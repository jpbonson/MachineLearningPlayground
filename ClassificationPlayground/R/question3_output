> library(caret) # for train/test partition
Loading required package: lattice
Loading required package: ggplot2
> library(randomForest) # randon forest classifier
randomForest 4.6-12
Type rfNews() to see new features/changes/bug fixes.
> library(klaR) # naive Bayes classifier
Loading required package: MASS
> 
> # REMEMBER: The indices start at 1! (not zero!)
> 
> # (c) Using 10-fold cross-validation, [1] train and evaluate a random forest and [2] a naÃ¯ve Bayes classifier. 
> # [3] Compare the accuracy of the two methods in terms of mean (ðœ‡) and standard deviation (ðœŽ) 
> #  of accuracy in 10 folds. 
> # [4] Eventually use a statistical significance test (e.g. studentâ€™s t-test) and determine whether the two 
> # methods are significantly different or not. Use ð›¼=0.05 as the significance threshold.
> 
> # [0] preparation
> 
> data <- read.csv(file="winequality_simple.csv", header=TRUE, sep=",", colClasses=c(rep('numeric', 11), 'factor'))
> set.seed(6000)
> trainIndex <- createDataPartition(data$quality, p = .7, list = FALSE, times = 1)
> train_data <- data[trainIndex,]
> test_data <- data[-trainIndex,]
> train_truth <- train_data[,12]
> train_data_no_labels <- train_data[,1:11]
> test_truth <- test_data[,12]
> test_data_no_labels <- test_data[,1:11]
> attach(train_data)
> train_control <- trainControl(method="repeatedcv",number=10, repeats=1)
> 
> # [1] train and evaluate a random forest using 10-fold cross-validation
> 
> model_rf <- train(train_data_no_labels, train_truth, trControl=train_control, method="rf")
> model_rf
Random Forest 

3413 samples
  11 predictors
   5 classes: '4', '5', '6', '7', '8' 

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 1 times) 
Summary of sample sizes: 3072, 3070, 3071, 3071, 3073, 3073, ... 
Resampling results across tuning parameters:

  mtry  Accuracy   Kappa      Accuracy SD  Kappa SD  
   2    0.6771247  0.4917172  0.02192085   0.03558397
   6    0.6736176  0.4898386  0.01823697   0.03033174
  11    0.6668735  0.4815670  0.01512147   0.02388600

Accuracy was used to select the optimal model using  the largest value.
The final value used for the model was mtry = 2. 
> test_pred_rf <- predict(model_rf, test_data_no_labels, type="raw")
> confusionMatrix(test_pred_rf, test_truth)
Confusion Matrix and Statistics

          Reference
Prediction   4   5   6   7   8
         4   6   0   2   0   0
         5  27 277  83   4   0
         6  13 157 543 134  21
         7   2   3  31 125  18
         8   0   0   0   1  13

Overall Statistics
                                          
               Accuracy : 0.6603          
                 95% CI : (0.6353, 0.6846)
    No Information Rate : 0.4514          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.4597          
 Mcnemar's Test P-Value : NA              

Statistics by Class:

                     Class: 4 Class: 5 Class: 6 Class: 7 Class: 8
Sensitivity          0.125000   0.6339   0.8240  0.47348 0.250000
Specificity          0.998584   0.8886   0.5943  0.95485 0.999290
Pos Pred Value       0.750000   0.7084   0.6256  0.69832 0.928571
Neg Pred Value       0.971074   0.8503   0.8041  0.89149 0.973029
Prevalence           0.032877   0.2993   0.4514  0.18082 0.035616
Detection Rate       0.004110   0.1897   0.3719  0.08562 0.008904
Detection Prevalence 0.005479   0.2678   0.5945  0.12260 0.009589
Balanced Accuracy    0.561792   0.7612   0.7091  0.71417 0.624645
> 
> # [2] train and evaluate a naÃ¯ve Bayes classifier using 10-fold cross-validation
> 
> model_nb <- train(train_data_no_labels, train_truth, trControl=train_control, method="nb")
There were 50 or more warnings (use warnings() to see the first 50)
> model_nb
Naive Bayes 

3413 samples
  11 predictors
   5 classes: '4', '5', '6', '7', '8' 

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 1 times) 
Summary of sample sizes: 3072, 3072, 3072, 3072, 3071, 3070, ... 
Resampling results across tuning parameters:

  usekernel  Accuracy   Kappa      Accuracy SD  Kappa SD  
  FALSE      0.4477018  0.2149006  0.02198806   0.02664114
   TRUE      0.4995739  0.2631456  0.01942608   0.02741945

Tuning parameter 'fL' was held constant at a value of 0
Accuracy was used to select the optimal model using  the largest value.
The final values used for the model were fL = 0 and usekernel = TRUE. 
> test_pred_nb <- predict(model_nb, test_data_no_labels, type="raw")
Warning messages:
1: In FUN(X[[i]], ...) :
  Numerical 0 probability for all classes with observation 370
2: In FUN(X[[i]], ...) :
  Numerical 0 probability for all classes with observation 834
3: In FUN(X[[i]], ...) :
  Numerical 0 probability for all classes with observation 867
4: In FUN(X[[i]], ...) :
  Numerical 0 probability for all classes with observation 1126
5: In FUN(X[[i]], ...) :
  Numerical 0 probability for all classes with observation 1171
> confusionMatrix(test_pred_nb, test_truth)
Confusion Matrix and Statistics

          Reference
Prediction   4   5   6   7   8
         4  10  13   7   0   1
         5  23 255 196  27   5
         6  13 149 301 103  16
         7   2  19 151 127  26
         8   0   1   4   7   4

Overall Statistics
                                          
               Accuracy : 0.4774          
                 95% CI : (0.4515, 0.5034)
    No Information Rate : 0.4514          
    P-Value [Acc > NIR] : 0.02442         
                                          
                  Kappa : 0.2258          
 Mcnemar's Test P-Value : 1.961e-06       

Statistics by Class:

                     Class: 4 Class: 5 Class: 6 Class: 7 Class: 8
Sensitivity          0.208333   0.5835   0.4568  0.48106  0.07692
Specificity          0.985127   0.7546   0.6492  0.83445  0.99148
Pos Pred Value       0.322581   0.5040   0.5172  0.39077  0.25000
Neg Pred Value       0.973408   0.8092   0.5923  0.87930  0.96676
Prevalence           0.032877   0.2993   0.4514  0.18082  0.03562
Detection Rate       0.006849   0.1747   0.2062  0.08699  0.00274
Detection Prevalence 0.021233   0.3466   0.3986  0.22260  0.01096
Balanced Accuracy    0.596730   0.6691   0.5530  0.65775  0.53420
> # tried using preprocessing to improve the results, but no significant result was obtained
> # warning: In density.default(xx, ...) : non-matched further arguments are disregarded
> # maybe the assumption of uncorrelated variables is wrong for this dataset
> 
> # [3] compare the accuracy and std. dev. of the two methods in the 10 folds
> # [REPORT]
> 
> # [4] determine if the methods are significantly different (0.05)
> rf_accs <- NULL
> nb_accs <- NULL
> for (i in 1:10) {
+     set.seed(i)
+     model_rf <- train(train_data_no_labels, train_truth, trControl=train_control, method="rf")
+     test_pred_rf <- predict(model_rf, test_data_no_labels, type="raw")
+     rf_acc <- confusionMatrix(test_pred_rf, test_truth)$overall['Accuracy']
+     rf_accs <- append(rf_accs, rf_acc)
+     model_nb <- train(train_data_no_labels, train_truth, trControl=train_control, method="nb", laplace = 1)
+     test_pred_nb <- predict(model_nb, test_data_no_labels, type="raw")
+     nb_acc <- confusionMatrix(test_pred_nb, test_truth)$overall['Accuracy']
+     nb_accs <- append(nb_accs, nb_acc)
+     # naive bayes is calculated based on the data, and dont use random variables, so the results dont change for different seeds
+ }
There were 50 or more warnings (use warnings() to see the first 50)
> rf_accs
 Accuracy  Accuracy  Accuracy  Accuracy  Accuracy  Accuracy  Accuracy  Accuracy 
0.6623288 0.6589041 0.6554795 0.6575342 0.6650685 0.6657534 0.6609589 0.6547945 
 Accuracy  Accuracy 
0.6623288 0.6561644 
> nb_accs
 Accuracy  Accuracy  Accuracy  Accuracy  Accuracy  Accuracy  Accuracy  Accuracy 
0.4773973 0.4773973 0.4773973 0.4773973 0.4773973 0.4773973 0.4773973 0.4773973 
 Accuracy  Accuracy 
0.4773973 0.4773973 
> wilcox.test(rf_accs,nb_accs)

	Wilcoxon rank sum test with continuity correction

data:  rf_accs and nb_accs
W = 100, p-value = 6.34e-05
alternative hypothesis: true location shift is not equal to 0

Warning message:
In wilcox.test.default(rf_accs, nb_accs) :
  cannot compute exact p-value with ties
>
